{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11b2aa45",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc494d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from os import listdir\n",
    "from matplotlib import pyplot as plt\n",
    "from random import shuffle\n",
    "from matplotlib import image as mpimg\n",
    "import cv2\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b109b361",
   "metadata": {},
   "source": [
    "#### Convert images to CSVs and split into training and validation sets\n",
    "\n",
    "In the process_image function a closing is a dilation followed by an erosion. As the name suggests, a closing is used to close holes inside of objects or for connecting components together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f813cf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_processing(image, closing_kernel=None, dilation_kernel=None):\n",
    "    if closing_kernel is None:\n",
    "        closing_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (4, 4))\n",
    "    if dilation_kernel is None:\n",
    "        dilation_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
    "    processed_image = image < 127\n",
    "    processed_image = processed_image.astype(np.uint8)\n",
    "    processed_image = cv2.morphologyEx(processed_image, cv2.MORPH_CLOSE, closing_kernel)\n",
    "    processed_image = cv2.dilate(processed_image, dilation_kernel)\n",
    "    return processed_image\n",
    "    \n",
    "    \n",
    "def data_processing(image_classes, latex_key, image_path, save_path, closing_kernel=None, dilation_kernel=None, data_split = 80):\n",
    "    if closing_kernel is None:\n",
    "        closing_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (4, 4))\n",
    "    if dilation_kernel is None:\n",
    "        dilation_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
    "    key = {}\n",
    "    for k, image_class in enumerate(image_classes):\n",
    "        sys.stdout.write(image_class + '\\n')\n",
    "        # Create key\n",
    "        if image_class in latex_key:\n",
    "            key[k] = latex_key[image_class]\n",
    "        else:\n",
    "            key[k] = image_class.lower()\n",
    "        \n",
    "        # Get files of images\n",
    "        images = []\n",
    "        image_files = listdir(image_path + image_class + '\\\\')\n",
    "        shuffle(image_files)\n",
    "        \n",
    "        for i, image_file in enumerate(image_files):\n",
    "            img = mpimg.imread(image_path + image_class + '\\\\' + image_file)\n",
    "            \n",
    "            # convert to binary, close and dilate\n",
    "            img = image_processing(img, closing_kernel, dilation_kernel)\n",
    "            \n",
    "            #add label\n",
    "            images.append(np.append(img.ravel(), k))\n",
    "            \n",
    "            #print progress\n",
    "            sys.stdout.write('\\r')\n",
    "            sys.stdout.write('{:.2%}'.format(i/len(image_files)))\n",
    "            sys.stdout.flush()\n",
    "        sys.stdout.write('\\r100.00%\\n')\n",
    "        \n",
    "        # split into training set and validation set and save\n",
    "        image_array = np.asarray(images)\n",
    "        ind = len(image_files)*data_split//100\n",
    "        np.savetxt(save_path + 'training\\\\' + image_class + '_tr' + '.csv', image_array[:ind], delimiter=',', fmt='%i')\n",
    "        np.savetxt(save_path + 'crossvalidation\\\\' + image_class + '_cv' + '.csv', image_array[ind:], delimiter=',', fmt='%i')\n",
    "        \n",
    "    #save key\n",
    "    if save_path:\n",
    "        with open(save_path + 'latex.csv', 'w') as csv_file:\n",
    "            writer = csv.writer(csv_file)\n",
    "            for k, v in key.items():\n",
    "                writer.writerow([k, v])\n",
    "    return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e920f995",
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_key = {'(': '\\\\left(',\n",
    "             ')': '\\\\right)',\n",
    "             'alpha': '\\\\alpha',\n",
    "             'ascii_124': '|',\n",
    "             'beta': '\\\\beta',\n",
    "             'Delta': '\\\\Delta',\n",
    "             'exists': '\\\\exists',\n",
    "             'forall': '\\\\forall',\n",
    "             'forward_slash': '\\\\',\n",
    "             'gamma': '\\\\gamma',\n",
    "             'gt' : '>',\n",
    "             'infty': '\\\\infty',\n",
    "             'int': '\\\\int',\n",
    "             'in': '\\\\in',\n",
    "             'lambda': '\\\\lambda',\n",
    "             'lt':'<',\n",
    "             'mu':'\\\\mu',\n",
    "             'neq': '\\\\neq',\n",
    "             'phi':'\\\\phi',\n",
    "             'pi':'\\\\pi',\n",
    "             'prime':'\\'',\n",
    "             'rightarrow':'\\\\rightarrow',\n",
    "             'sigma':'\\\\sigma',\n",
    "             'sqrt':'\\\\sqrt',\n",
    "             'sum':'\\\\sum',\n",
    "             'theta': '\\\\theta',\n",
    "             'times':'\\\\times',\n",
    "             '[':'\\\\left[',\n",
    "             ']':'\\\\right]',\n",
    "             '{':'\\\\left{',\n",
    "             '}':'\\\\right}'}\n",
    "             \n",
    "\n",
    "# convert images to csv\n",
    "img_path = \"C:\\\\Users\\\\sivuy\\\\Desktop\\\\My Final Year Project\\\\data\\\\extracted_images\\\\\"\n",
    "img_classes = listdir(img_path)\n",
    "save_path = \"C:\\\\Users\\\\sivuy\\\\Desktop\\\\My Final Year Project\\\\data\\\\csv\\\\\"\n",
    "key = data_processing(img_classes, latex_key, img_path, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407803f8",
   "metadata": {},
   "source": [
    "# The raw data from Kaggle looks like the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466316a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"C:\\\\Users\\\\sivuy\\\\Desktop\\\\My Final Year Project\\\\data\\\\extracted_images\\\\\"\n",
    "image_classes = listdir(img_path)\n",
    "shuffle(image_classes)\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(10, 10))\n",
    "for ax, image_class in zip(axes.ravel(), image_classes[:9]):\n",
    "    image_file = listdir(img_path + image_class + '\\\\')[0]\n",
    "    image = mpimg.imread(img_path + image_class + '\\\\' + image_file)\n",
    "    ax.imshow(image, cmap='gray')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(image_class)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69778e17",
   "metadata": {},
   "source": [
    "The dataset was not the highest quality, some basic image processing was required. Using the functions above, each image is converted into binary, morphed so that the holes/missing pixels were closed and then dilated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3818fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize=(10, 10))\n",
    "for ax, image_class in zip(axes.ravel(), image_classes[:9]):\n",
    "    image_file = listdir(img_path + image_class + '\\\\')[0]\n",
    "    image = mpimg.imread(img_path + image_class + '\\\\' + image_file)\n",
    "    new_image = image_processing(image)\n",
    "    ax.imshow(new_image, cmap='gray')\n",
    "    ax.axis('off')\n",
    "    ax.set_title(image_class)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fdf764",
   "metadata": {},
   "source": [
    "## Combine all files and shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796a129f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_files(image_classes, training_path, save_path, shuffle=True):\n",
    "    for path, ending in [(training_path, '_tr.csv'), (crossvalidation_path, '_cv.csv')]:\n",
    "        all_files = None\n",
    "        for i, image_class in enumerate(image_classes):\n",
    "            #print progress\n",
    "            sys.stdout.write('\\r')\n",
    "            sys.stdout.write('{:.2%}'.format(i/len(image_classes)))\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "            current_class = np.loadtxt(path + image_class + ending, delimiter=',')\n",
    "            if all_files is None:\n",
    "                all_files = current_class.copy()\n",
    "                continue\n",
    "            all_files = np.vstack([all_files, current_class])\n",
    "        sys.stdout.write('\\r100.00%\\n')\n",
    "        \n",
    "        # Shuffle\n",
    "        if shuffle: np.random.shuffle(all_files)\n",
    "        np.savetxt(save_path + 'all' + ending, all_files, delimiter=',', fmt='%i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1d48e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"C:\\\\Users\\\\sivuy\\\\Desktop\\\\My Final Year Project\\\\data\\\\extracted_images\\\\\"\n",
    "image_classes = listdir(img_path)\n",
    "\n",
    "save_path = \"C:\\\\Users\\\\sivuy\\\\Desktop\\\\My Final Year Project\\\\data\\\\csv\\\\\"\n",
    "training_path = \"C:\\\\Users\\\\sivuy\\\\Desktop\\\\My Final Year Project\\\\data\\\\csv\\\\training\\\\\"\n",
    "crossvalidation_path = \"C:\\\\Users\\\\sivuy\\\\Desktop\\\\My Final Year Project\\\\data\\\\csv\\\\crossvalidation\\\\\"\n",
    "\n",
    "combine_files(image_classes, training_path, crossvalidation_path, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2ecf18",
   "metadata": {},
   "source": [
    "## Image Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab50129",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import image as mpimg\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce437f9",
   "metadata": {},
   "source": [
    "### Binarize Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7defcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_processing(input_file,dilation_kernel=None, dilation_iterations=1):\n",
    "    #Binary\n",
    "    processed_image = cv2.cvtColor(cv2.imread(input_file_name).astype(np.uint8), cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    #Remove initial noise and smoothen lighting gradient\n",
    "    processed_image = cv2.GaussianBlur(processed_image, (11, 11), 0)\n",
    "    \n",
    "    #Threshold\n",
    "    processed_image = cv2.adaptiveThreshold(processed_image, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 0.001)\n",
    "    \n",
    "    #Blur\n",
    "    processed_image = cv2.blur(processed_image, (3, 3))\n",
    "    \n",
    "    #Threshold\n",
    "    processed_image = cv2.threshold(processed_image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
    "    \n",
    "    #Median blur\n",
    "    processed_image = cv2.medianBlur(processed_image, 17)\n",
    "    \n",
    "    #Threshold\n",
    "    processed_image_bw = 1 - np.round(processed_image/255)\n",
    "    \n",
    "    #Dilate\n",
    "    if not dilation_kernel:\n",
    "        dilation_kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "    processed_image_bw = cv2.dilate(processed_image_bw, dilation_kernel, iterations=dilation_iterations)\n",
    "    \n",
    "    return processed_image_bw.astype(np.uint8)\n",
    "\n",
    "# Read in image\n",
    "input_file_name = 'sqrt1.jpg'\n",
    "input_file = mpimg.imread(input_file_name)[:, :, 0:3].reshape(-1, 3)\n",
    "new_shape = list(mpimg.imread(input_file_name).shape)\n",
    "new_shape[2] = 3\n",
    "input_file = input_file.reshape(new_shape)\n",
    "\n",
    "input_file_processed = input_processing(input_file)\n",
    "\n",
    "plt.imshow(input_file)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(input_file_processed, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fb84d8",
   "metadata": {},
   "source": [
    "### Break into connected components\n",
    "\n",
    "After the entire image is processed, the image is broken into connected components as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fcbbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, labels = cv2.connectedComponents(input_file_processed)\n",
    "\n",
    "plt.imshow(labels)\n",
    "plt.axis('off')\n",
    "plt.savefig('C:\\\\Users\\\\sivuy\\\\anaconda3\\\\lib\\\\site-packages\\\\pywebio\\\\html\\\\expression.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d23ee0e",
   "metadata": {},
   "source": [
    "### Process single character components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014469bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(arr, pad_width):\n",
    "    arr_new = np.hstack([np.zeros([arr.shape[0], pad_width]), arr]) #left\n",
    "    arr_new = np.hstack([arr_new, np.zeros([arr_new.shape[0], pad_width])]) #right\n",
    "    arr_new = np.vstack([np.zeros([pad_width, arr_new.shape[1]]), arr_new]) #up\n",
    "    arr_new = np.vstack([arr_new, np.zeros([pad_width, arr_new.shape[1]])]) #down\n",
    "    return arr_new\n",
    "\n",
    "def square(arr, pad_width, top, left, bottom, right):\n",
    "    arr_square = arr[top-pad_width:bottom+pad_width, left-pad_width:right+pad_width]\n",
    "    diff = abs(arr_square.shape[1] - arr_square.shape[0])\n",
    "    pad = diff//2\n",
    "    if arr_square.shape[0] < arr_square.shape[1]:\n",
    "        arr_square = np.vstack([np.zeros([pad, arr_square.shape[1]]), arr_square]) #up\n",
    "        arr_square = np.vstack([arr_square, np.zeros([pad + (diff % 2 == 1), arr_square.shape[1]])]) #down\n",
    "    elif arr_square.shape[0] >= arr_square.shape[1]:\n",
    "        arr_square = np.hstack([np.zeros([arr_square.shape[0], pad]), arr_square]) #left\n",
    "        arr_square = np.hstack([arr_square, np.zeros([arr_square.shape[0], pad + (diff % 2 == 1)])]) #right\n",
    "    return arr_square\n",
    "\n",
    "def erode(arr, erosion_percentage):\n",
    "    for dimensions in range(1, 12):\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (dimensions, dimensions))\n",
    "        erosion = cv2.erode(arr, kernel, iterations = 1)\n",
    "        if np.sum(erosion)/np.sum(arr) < erosion_percentage:\n",
    "            break\n",
    "    dimensions -= 1\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (dimensions, dimensions))\n",
    "    erosion = cv2.erode(arr, kernel, iterations = 1)\n",
    "    return erosion\n",
    "    \n",
    "def get_components(labels, pad_width=3, erosion_percent=0.4, show=True):\n",
    "    components = {i:{'label': None,\n",
    "                    'output': None,\n",
    "                    'top_left': None,\n",
    "                    'bottom_right': None,\n",
    "                    'image':None,\n",
    "                    'group': None,\n",
    "                    'sup': False,\n",
    "                    'sub': False}\n",
    "                  for i in range(1, len(np.unique(labels)))}\n",
    "    fig, axes = plt.subplots(2, int((len(components)+1)/2), figsize=(15, 5))\n",
    "    for i, ax in zip(sorted(components.keys()), axes.ravel()):\n",
    "        label = labels.copy()\n",
    "        label[labels !=i] = 0\n",
    "        label_padded = pad(label, pad_width)\n",
    "        \n",
    "        # Get dimentions of component\n",
    "        xs, ys = np.where(label != 0)\n",
    "        top, bottom, left, right = np.min(xs), np.max(xs), np.min(ys), np.max(ys)\n",
    "        components[i]['top_left'] = (top, left)\n",
    "        components[i]['bottom_right'] = (bottom, right)\n",
    "        \n",
    "        # Square and resize\n",
    "        label_square = square(label_padded, pad_width, top, left, bottom, right)\n",
    "        label_square = cv2.resize(label_square, (45, 45))\n",
    "        label_square[label_square != 0] = 1\n",
    "        \n",
    "        # Erode based on size\n",
    "        label_eroded = erode(label_square, erosion_percent)\n",
    "        components[i]['image'] = label_eroded.ravel()\n",
    "        \n",
    "        if show:\n",
    "            ax.imshow(label_eroded, cmap='gray')\n",
    "            ax.axis('off')\n",
    "    if show: plt.show()\n",
    "    return components\n",
    "\n",
    "components = get_components(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179fe8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(labels)\n",
    "for tl, br in [(components[i]['top_left'], components[i]['bottom_right']) for i in components]:\n",
    "    plt.plot(*tl[::-1], 'rx')\n",
    "    plt.plot(*br[::-1], 'rx')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dbd78b",
   "metadata": {},
   "source": [
    "# LeNet architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3835f04a",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942331e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036b83ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tr = np.loadtxt('C:\\\\Users\\\\sivuy\\\\Desktop\\\\My Final Year Project\\\\data\\\\csv\\\\crossvalidation\\\\all_tr.csv', delimiter=',', dtype=int)\n",
    "data_cv = np.loadtxt('C:\\\\Users\\\\sivuy\\\\Desktop\\\\My Final Year Project\\\\data\\\\csv\\\\crossvalidation\\\\all_cv.csv', delimiter=',', dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bc3a0f",
   "metadata": {},
   "source": [
    "#### Define network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a30069",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model(features, labels, mode):\n",
    "    \"\"\"Model function for CNN\"\"\"\n",
    "    #Input layer\n",
    "    input_layer = tf.reshape(features[\"x\"], [-1, 45, 45, 1])\n",
    "    \n",
    "    # Convolutional Layer #1\n",
    "    conv1 = tf.compat.v1.layers.conv2d(\n",
    "        inputs = input_layer,\n",
    "        filters = 6,\n",
    "        kernel_size = [4, 4],\n",
    "        padding = \"valid\",\n",
    "        activation = tf.nn.relu)\n",
    "    \n",
    "    # Pooling layer #1\n",
    "    pool1 = tf.compat.v1.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "    \n",
    "    # Convolutional layer #2 and pooling layer #2\n",
    "    conv2 = tf.compat.v1.layers.conv2d(\n",
    "        inputs = pool1,\n",
    "        filters = 12,\n",
    "        kernel_size = [4, 4],\n",
    "        padding = \"valid\",\n",
    "        activation = tf.nn.relu)\n",
    "    \n",
    "    # Pooling layer #2\n",
    "    pool2 = tf.compat.v1.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "    \n",
    "    # Dense layer #1\n",
    "    pool2_flat = tf.reshape(pool2, [-1, 972])\n",
    "    dense1 = tf.compat.v1.layers.dense(inputs=pool2_flat, units=240, activation = tf.nn.relu)\n",
    "    \n",
    "    # Desne layer #2\n",
    "    pool2_flat = tf.reshape(pool2, [-1, 240])\n",
    "    dense2 = tf.compat.v1.layers.dense(inputs=dense1, units=120, activation = tf.nn.relu)\n",
    "    \n",
    "    # Logits layer\n",
    "    logits = tf.compat.v1.layers.dense(inputs=dense2, units=68)\n",
    "    \n",
    "    predictions = {\n",
    "        # Generate predictions (for PREDICT and EVAL mode)\n",
    "        \"classes\": tf.math.argmax(input=logits, axis=1),\n",
    "        # Add 'softmax_tensor' to the graph. It is used for PREDICT and by the\n",
    "        # 'logging_hook'\n",
    "        \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "    }\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "    \n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    onehot_labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=68)\n",
    "    loss = tf.compat.v1.losses.softmax_cross_entropy(\n",
    "        onehot_labels = onehot_labels, logits=logits)\n",
    "    \n",
    "    # Configure the Training Op (for Train mode)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.compat.v1.train.GradientDescentOptimizer(learning_rate=1e-2)\n",
    "        train_op = optimizer.minimize(\n",
    "            loss = loss,\n",
    "            global_step = tf.compat.v1.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "    \n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    eval_metric_ops = {\n",
    "        \"accuracy\": tf.compat.v1.metrics.accuracy(\n",
    "            labels=labels, predictions=predictions[\"classes\"])}\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22f95a7",
   "metadata": {},
   "source": [
    "#### Train on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f445942",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_tr[:, :-1].astype(np.float32)\n",
    "train_labels = data_tr[:, -1].astype(np.int32)\n",
    "\n",
    "eval_data = data_cv[:, : -1].astype(np.float32)\n",
    "eval_labels = data_cv[:, -1].astype(np.int32)\n",
    "\n",
    "classifier = tf.estimator.Estimator(\n",
    "    model_fn = cnn_model, model_dir=\"cnn_model\")\n",
    "\n",
    "# Train the model\n",
    "train_input_fn = tf.compat.v1.estimator.inputs.numpy_input_fn(x={\"x\": train_data},y=train_labels,batch_size=250,num_epochs=125, shuffle=True)\n",
    "classifier.train(input_fn=train_input_fn)\n",
    "    \n",
    "# Evaluate the model and print results\n",
    "eval_input_fn = tf.compat.v1.estimator.inputs.numpy_input_fn(x={\"x\":eval_data}, y=eval_labels, num_epochs=1, shuffle=False)\n",
    "eval_results = classifier.evaluate(input_fn=eval_input_fn)\n",
    "print(eval_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8924ac",
   "metadata": {},
   "source": [
    "## Output of LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6242daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('C:\\\\Users\\\\sivuy\\\\Desktop\\\\My Final Year Project\\\\data\\\\csv\\\\latex.csv', delimiter=',', header=None)\n",
    "\n",
    "key = dict(zip(df.iloc[:, 0], df.iloc[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a11a640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "classifier = tf.estimator.Estimator(\n",
    "    model_fn = cnn_model, model_dir=\"cnn_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c95e4c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def classify(components, classifier):\n",
    "    test = np.asarray([components[i]['image'] for i in sorted(components.keys())]).astype(np.float32)\n",
    "    predict_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        x={\"x\":test},\n",
    "        y=None,\n",
    "        batch_size=1,\n",
    "        num_epochs=1,\n",
    "        shuffle=False,\n",
    "        num_threads=1)\n",
    "    predict_results = classifier.predict(predict_input_fn)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, int((len(components) + 1)/2), figsize=(15, 5))\n",
    "    \n",
    "    for i, (d, ax) in enumerate(zip(predict_results, axes.ravel())):\n",
    "        components[i + 1]['label'] = d['classes']\n",
    "        components[i + 1]['output'] = key[d['classes']]\n",
    "        \n",
    "        ax.imshow(components[i + 1]['image'].reshape(45, 45), cmap='gray')\n",
    "        ax.axis('off')\n",
    "        ax.set_title('Classified as ' + key[d['classes']])\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "classify(components, classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34a7265",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_group(components, offset_threshold=3):\n",
    "    heights = [[components[i]['top_left'][0], components[i]['bottom_right'][0]] for i in components]\n",
    "    groups = [heights[0]]\n",
    "    for height in heights[1:]:\n",
    "        if height[0] + offset_threshold < groups[-1][1]:\n",
    "            groups[-1][1] = max(height[1], groups[-1][1])\n",
    "        else:\n",
    "            groups.append(height)\n",
    "    for i in components:\n",
    "        for group in groups:\n",
    "            if group[0] < components[i]['top_left'][0] + offset_threshold < group[1]:\n",
    "                components[i]['group'] = group\n",
    "    return components, groups\n",
    "\n",
    "components, groups = assign_group(components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41603be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_script(components, groups):\n",
    "    for group in groups:\n",
    "        bottoms = [components[i]['bottom_right'][0] for i in sorted(components.keys()) if components[i]['group'] == group]\n",
    "        tops = [components[i]['top_left'][0] for i in sorted(components.keys()) if components[i]['group'] == group]\n",
    "        bottoms_mean = np.mean(bottoms)\n",
    "        bottoms_std = np.std(bottoms)\n",
    "        tops_mean = np.mean(tops)\n",
    "        tops_std = np.std(tops)\n",
    "        \n",
    "        if len(bottoms) == 1:\n",
    "            continue\n",
    "        for component in components:\n",
    "            if components[component]['group'] == group:\n",
    "                s = (bottoms_mean - components[component]['bottom_right'][0]/bottoms_std - (components[component]['top_left'][0] -tops_mean)/tops_std)\n",
    "                if s > 2.5:\n",
    "                    components[component]['sup'] = True\n",
    "                elif s < -2.5:\n",
    "                    components[component]['sub'] = True\n",
    "    return components\n",
    "\n",
    "identify_script(components, groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3a1ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_latex(components, groups):\n",
    "    lr_order = sorted(components.keys(), key=lambda x: components[x]['top_left'][1])\n",
    "    vsep = {tuple(group):[] for group in groups}\n",
    "    MODE_SUP = set()\n",
    "    MODE_SUB = set()\n",
    "    MODE_SQRT = {}\n",
    "    \n",
    "    for l in lr_order:\n",
    "        t, left = components[l]['top_left']\n",
    "        b, right = components[l]['bottom_right']\n",
    "        for g in vsep:\n",
    "            if g[0] <= t <=b <= g[1]:\n",
    "                \n",
    "                if g in MODE_SQRT and left > MODE_SQRT[g]:\n",
    "                    vsep[g].append('}')\n",
    "                    del MODE_SQRT[g]\n",
    "                \n",
    "                if g in MODE_SUP and not components[l]['sup']:\n",
    "                    vsep[g].append('}')\n",
    "                    MODE_SUP.remove(g)\n",
    "                    \n",
    "                if g in MODE_SUB and not components[l]['sub']:\n",
    "                    vsep[g].append('}')\n",
    "                    MODE_SUB.remove(g)\n",
    "                    \n",
    "                if g not in MODE_SUP and components[l]['sup']:\n",
    "                    vsep[g].append('^{')\n",
    "                    MODE_SUP.add(g)\n",
    "                \n",
    "                if g not in MODE_SUB and components[l]['sub']:\n",
    "                    vsep[g].append('_{')\n",
    "                    MODE_SUB.add(g)\n",
    "                    \n",
    "                vsep[g].append(components[l]['output'] + ' ')\n",
    "                if components[l]['output'] == '\\\\sqrt':\n",
    "                    MODE_SQRT[g] = right\n",
    "                    vsep[g].append('{')\n",
    "                    \n",
    "                break\n",
    "    for i in MODE_SQRT:\n",
    "        vsep[g].append('}')\n",
    "    for g in vsep:\n",
    "        vsep[g] = ''.join(vsep[g])\n",
    "\n",
    "    if len(vsep) == 3:\n",
    "        first_g, _, last_g = list(sorted([g for g in vsep], key = lambda g:g[0]))\n",
    "        final = '\\\\frac{' + vsep[first_g] + '}{' + vsep[last_g] + '}'\n",
    "    else:\n",
    "        final = list(vsep.values())[0]\n",
    "    final = '$' + final + '$'\n",
    "    return final\n",
    "\n",
    "expression = construct_latex(components, groups)\n",
    "expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb47e34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bdb272",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.text(0.5, 0.5, expression, size=50)\n",
    "ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4958d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from latex2sympy2 import latex2sympy, latex2latex\n",
    "from sympy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6126a570",
   "metadata": {},
   "outputs": [],
   "source": [
    "expression = r\"5x + x\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d126feef",
   "metadata": {},
   "outputs": [],
   "source": [
    "sympyexp = latex2sympy(expression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688adaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(sympyexp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48805ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "simplify(sympyexp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3189777f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "img=Image.open('example.png')\n",
    "a=img.convert(\"P\", palette=Image.ADAPTIVE, colors=24)\n",
    "a.save('example2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ef3867",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pywebio import start_server\n",
    "import pywebio.input\n",
    "from pywebio.output import *    \n",
    "from pywebio.session import *\n",
    "from pywebio.pin import *\n",
    "from pywebio import STATIC_PATH\n",
    "\n",
    "def main():\n",
    "    imgs = pywebio.input.file_upload(\"Select some pictures:\", accept=\"image/*\", multiple=True)\n",
    "    for img in imgs:\n",
    "        put_image(img['content'], width='500px')\n",
    "    camera_image_name = imgs[0]['filename']\n",
    "    isthombe = open(STATIC_PATH + '\\expression.png', 'rb').read()  \n",
    "    put_image(isthombe, width='500px')\n",
    "    put_markdown(str(expression))\n",
    "    put_markdown(str(simplify(sympyexp)))\n",
    "    #pywebio.output.put_image(\"example2.png\")\n",
    "    #popup('Calculation done')\n",
    "\n",
    "start_server(main, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206346ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_name = \"WhatsApp Image.jpeg\"\n",
    "processed_image = cv2.cvtColor(cv2.imread(input_file_name).astype(np.uint8), cv2.COLOR_BGR2GRAY)\n",
    "processed_image = cv2.GaussianBlur(processed_image, (11, 11), 0)\n",
    "processed_image = cv2.adaptiveThreshold(processed_image, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 0.001)\n",
    "processed_image = cv2.blur(processed_image, (3, 3))\n",
    "processed_image = cv2.threshold(processed_image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
    "processed_image = cv2.medianBlur(processed_image, 17)\n",
    "processed_image = 1 - np.round(processed_image/255)\n",
    "plt.imshow(processed_image,cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0bcae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "img_path = \"C:\\\\Users\\\\sivuy\\\\Desktop\\\\My Final Year Project\\\\data\\\\extracted_images\\\\-\"\n",
    "count = 0\n",
    "klas = \"-\"\n",
    "# for image in listdir(img_path):\n",
    "#     img_read = mpimg.imread(img_path + \"\\\\\" + image)\n",
    "#     new_image = image_processing(img_read)\n",
    "#     plt.imshow(new_image,cmap='gray')\n",
    "#     plt.title(image)\n",
    "#     plt.axis('off')\n",
    "#     plt.show()\n",
    "#     count =  count + 1\n",
    "#     if count > 2:\n",
    "#         break\n",
    "image = listdir(img_path)[0]\n",
    "img_read = mpimg.imread(img_path + \"\\\\\" + image)\n",
    "new_image = image_processing(img_read)\n",
    "img_as_array = np.asarray(new_image).astype(np.float32)\n",
    "\n",
    "\n",
    "\n",
    "test = img_as_array\n",
    "predict_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "     x={\"x\":test},\n",
    "     y=None,\n",
    "     batch_size=1,\n",
    "     num_epochs=1,\n",
    "     shuffle=False,\n",
    "     num_threads=1)\n",
    "predict_results = classifier.predict(predict_input_fn)\n",
    "for prediction in predict_results:\n",
    "    print('result: {}'.format(prediction))\n",
    "\n",
    "#predictions = list(classifier.predict(input_fn=predict_input_fn))\n",
    "#predicted_classes = [p[\"classes\"] for p in predictions]\n",
    "\n",
    "#print(\"New Samples, Class Predictions:    {}\\n\".format(predicted_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1488ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17b2af2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
